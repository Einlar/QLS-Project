{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verified-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import mpl_interactions.ipyplot as iplt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union, List\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from mpl2latex import mpl2latex, latex_figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "congressional-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioNeurons():\n",
    "    \"\"\"\n",
    "    Simulate synaptic plasticity rule and dynamic equations from [1].\n",
    "    \n",
    "    [1]: \"Unsupervised learning by competing hidden units\", D. Krotov, J. J. Hopfield, 2019, \n",
    "         https://www.pnas.org/content/116/16/7723\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_dim : int = 2,\n",
    "                 output_dim : int = 4,\n",
    "                 w_inh : float = .63,\n",
    "                 delta : float = .4,\n",
    "                 h_star : float = 0.1,\n",
    "                 tau : float = 1,\n",
    "                 tau_L : float = 100,\n",
    "                 lebesgue_p : float = 4.):\n",
    "        \"\"\"\n",
    "        Set all the parameters for the simulation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of visible units\n",
    "        output_dim : int\n",
    "            Number of hidden units\n",
    "        w_inh : float\n",
    "            Strength of global inhibition (from eq. 8 in [1]). Should be >= 0, and high enough so that\n",
    "            only a small fraction of activations are positive in the steady-state (i.e. self.stationary_activations returns\n",
    "            a small percentage of positive values).\n",
    "        delta : float\n",
    "            Strength of anti-Hebbian learning (from eq. 9 in [1]). \n",
    "        h_star : float\n",
    "            Threshold for activation (from eq. 9 in [1])\n",
    "        tau : float\n",
    "            Dynamical time scale of individual neurons (from eq. 8 in [1])\n",
    "        tau_L : float\n",
    "            Time scale of learning dynamics (from eq. 3 in [1]). Should be >> tau.\n",
    "        lebesgue_p : float\n",
    "            Parameter for Lebesgue measure, used for defining an inner product (from eq. 2 in [1]).\n",
    "        \"\"\"\n",
    "        \n",
    "        #Store parameters\n",
    "        self.input_dim  = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.w_inh      = w_inh\n",
    "        self.delta      = delta\n",
    "        self.h_star     = h_star\n",
    "        self.tau        = tau\n",
    "        self.tau_L      = tau_L\n",
    "        self.lebesgue_p = lebesgue_p\n",
    "        \n",
    "        #Set initial state\n",
    "        self.activations = np.zeros(output_dim, dtype=float)\n",
    "        self.weights     = np.random.normal(size=(output_dim, input_dim)) #normal size\n",
    "        \n",
    "        print(str(self))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"BioNeurons(input_dim={self.input_dim}, output_dim={self.output_dim}, w_inh={self.w_inh}, delta={self.delta}, lebesgue_p={self.lebesgue_p}, h_star={self.h_star}, tau={self.tau}, tau_L={self.tau_L})\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    @jit\n",
    "    def forward(self, inputs : np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sets the visible units to `inputs`, and propagates this signal through the weights to compute the network's output.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray of shape (self.input_dim,)\n",
    "            Vector with values for the input neurons\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray of shape (self.output_dim,)\n",
    "            Vector with values for the hidden units (before activation).\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot(self.weights, inputs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def currents(inputs : np.ndarray, weights : np.ndarray, lebesgue_p : float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes currents at each hidden neuron (eq. 8 from [1]). \n",
    "        \n",
    "        Formula is:\n",
    "        $$ I_\\mu = <W, v>_\\mu = \\sum_i sgn(W_{\\mu i}) |W_{\\mu i}|^{p-1} v_i $$\n",
    "        where p is self.lebesgue_p\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray of shape (self.input_dim,)\n",
    "            Vector with values for the input neurons\n",
    "        weights : np.ndarray of shape (self.output_dim, self.input_dim)\n",
    "            Weights\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        currents : np.ndarray of shape (self.output_dim,)\n",
    "            Vector with currents at each of the hidden neurons\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot( np.sign(weights) * np.abs(weights) ** (lebesgue_p - 1), inputs )\n",
    "    \n",
    "    def dh_dt(self,\n",
    "              t : float,\n",
    "              y : np.ndarray,\n",
    "              inputs : np.ndarray,\n",
    "              weights : np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the right hand side of eq. 8 from [1], i.e. the time derivative of all neuron activations.\n",
    "        \n",
    "        Formula is:\n",
    "            $$dh_\\mu/dt = I_\\mu - w_{inh} \\sum_{\\nu \\neq \\mu} \\max(h_\\nu, 0) - h_\\mu $$ \n",
    "        where $I_\\mu$ are the currents at the hidden neurons, and $h_\\mu$ are their activations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : float\n",
    "            Time instant, needed as interface to scipy.solve_ivp. \n",
    "        y : np.ndarray of shape (self.output_dim,)\n",
    "            Value of hidden neurons\n",
    "        inputs : np.ndarray of shape (self.input_dim,)\n",
    "            Value of input neurons\n",
    "        weights : np.ndarray of shape (self.output_dim, self.input_dim)\n",
    "            Weights for the network\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dh_dt : np.ndarray of shape (self.output_dim,)\n",
    "            Vector containing the time derivative of each hidden neuron activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        activations = y\n",
    "        currents = self.currents(inputs, weights, lebesgue_p=self.lebesgue_p)\n",
    "        \n",
    "        positive_activations = activations * (activations > 0) #Set to 0 all the non-positive activations\n",
    "        global_inhibition = np.sum(positive_activations) - positive_activations #Remove \"self\" activation by each term\n",
    "        \n",
    "        return (currents - self.w_inh * global_inhibition - activations) / self.tau\n",
    "    \n",
    "    def stationary_activations(self,\n",
    "                               inputs : np.ndarray,\n",
    "                               weights : np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the hidden neuron activations at stationarity, for a given value of the input neurons and weights of connections.\n",
    "        For simplicity, this is done by numerically solving the differential equation (8 in [1]) for a some large time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray of shape (self.input_dim,)\n",
    "            Values for the input neurons\n",
    "        weights : np.ndarray of shape (self.output_dim, self.input_dim)\n",
    "            Weights\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h* : np.ndarray of shape (self.output_dim,)\n",
    "            Vector with the activations of the hidden neurons at stationarity\n",
    "        \"\"\"\n",
    "    \n",
    "        large_time = 5 * self.tau\n",
    "        \n",
    "        sol = solve_ivp(neurons.dh_dt, [0, large_time], np.zeros(self.output_dim), args=(inputs, weights), t_eval = [large_time])\n",
    "        #Stationary solution does not depend on the initual condition, which is here set to 0.\n",
    "        \n",
    "        return sol.y.flatten()\n",
    "    \n",
    "    @staticmethod\n",
    "    def g(activations : np.ndarray, h_star : float = 0.8, delta : float = 0.4) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Activation function for training, implementing temporal competition between the patterns (eq. 9 from [1]). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : np.ndarray of shape (self.output_dim,)\n",
    "            Value of post-synaptic (hidden) neurons\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        post_act : np.ndarray of shape (self.output_dim,)\n",
    "            Post-activation values for the hidden neurons.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.where(activations < 0, 0, np.where(activations <= h_star, -delta, 1.))\n",
    "    \n",
    "    def dW_dt(self,\n",
    "              t : float,\n",
    "              y : np.ndarray,\n",
    "              inputs : np.ndarray,\n",
    "              flatten : bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the right hand side of eq. 3 from [1], i.e. the time derivative of the weights. \n",
    "        It is computed in the quasi-stationary approximation, i.e. by setting the hidden units activations to their \n",
    "        stationary value at each step (which is a good approximation since the timescale for weight evolution is\n",
    "        much larger than that of individual neuron dynamics).\n",
    "        \n",
    "        Formula is:\n",
    "            $$dW_{\\mu i}/dt = [g(h_\\mu) (v_i - I_\\mu W_{\\mu i})] / \\tau_L$$\n",
    "        where $v_i$ are the visible units, $h_\\mu$ the hidden ones, $W_{\\mu i}$ are the weights, and $I_\\mu$ are the currents.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : float\n",
    "            Time instant, needed as interface to scipy.solve_ivp. \n",
    "        y : np.ndarray of shape (self.output_dim, self.input_dim), or (self.output_dim * self.input_dim,) if flatten=True\n",
    "            Weights\n",
    "        inputs : np.ndarray of shape (self.input_dim,)\n",
    "            Value of input neurons\n",
    "        flatten : bool\n",
    "            If True, the output will be flattened to a 1D vector, which is compatible with scipy.solve_ivp.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dW_dt : np.ndarray of shape (self.output_dim, self.input_dim), or (self.output_dim * self.input_dim,) if flatten=True\n",
    "            Time derivatives of weights\n",
    "        \"\"\"\n",
    "        \n",
    "        if flatten:\n",
    "            weights = y.reshape(*self.weights.shape)\n",
    "        else:\n",
    "            weights = y\n",
    "            \n",
    "        #currents = self.currents(inputs, weights, lebesgue_p=self.lebesgue_p)\n",
    "        \n",
    "        activations = self.stationary_activations(inputs, weights)\n",
    "        #Quasi-stationary approximation: use stationary value of activations, since their evolution is much faster\n",
    "        \n",
    "        post_activations = self.g(activations, h_star=self.h_star, delta=self.delta)\n",
    "        \n",
    "        result = (np.outer(post_activations, inputs) - ((post_activations * activations).reshape(-1, 1) * weights)) / self.tau_L\n",
    "        \n",
    "        return result.flatten() if flatten else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strategic-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeurons(input_dim=2, output_dim=4, w_inh=0.3, delta=0.4, lebesgue_p=4.0, h_star=0.1, tau=1, tau_L=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.15526475,  1.64196229, -0.17358915, -5.43402091])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tests to see if the above functions work\n",
    "neurons = BioNeurons(w_inh=.3)\n",
    "neurons.dh_dt(None, neurons.activations, np.random.normal(size=2), neurons.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "found-richardson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.73295296, -1.63071772, -0.57035217,  2.5805892 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons.stationary_activations(np.random.normal(size=2), neurons.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "missing-retirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeurons(input_dim=2, output_dim=10, w_inh=0.63, delta=0.4, lebesgue_p=4.0, h_star=0.1, tau=1, tau_L=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab1dd926a4c4742a1f2eff39b81177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c8a4f8c770457cb49fa65fb85afa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='w_inh', max=99, readout=False), Label(value='0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize effect of w_inh\n",
    "neurons = BioNeurons(output_dim=10)\n",
    "\n",
    "t_end = 5\n",
    "ts = np.linspace(0, t_end, 100)\n",
    "\n",
    "x = np.random.rand(2)\n",
    "def activation_evolution(t, w_inh, lebesgue_p):\n",
    "    neurons.w_inh = w_inh\n",
    "    neurons.lebesgue_p = lebesgue_p\n",
    "    \n",
    "    sol = solve_ivp(neurons.dh_dt, [0, t_end], neurons.activations, args=(x, neurons.weights), t_eval = t)\n",
    "    \n",
    "    return sol.y.T\n",
    "\n",
    "\n",
    "p_values = np.arange(2, 8)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "controls = iplt.plot(ts, activation_evolution, w_inh=(0, 1, 100), lebesgue_p=p_values, label=[f\"$h_{i}$\" for i in range(neurons.output_dim)])\n",
    "_ = plt.legend()\n",
    "plt.xlabel(\"Time $t$\")\n",
    "plt.ylabel(\"Activations $h_\\mu$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "responsible-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeurons(input_dim=2, output_dim=15, w_inh=0.3, delta=0.4, lebesgue_p=2.0, h_star=0.8, tau=1, tau_L=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cfac7c8535494aa840dc9c5321d884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Save plot\n",
    "n_neurons = 15\n",
    "neurons = BioNeurons(output_dim=n_neurons, w_inh=.3, h_star=.8, lebesgue_p=2.)\n",
    "\n",
    "t_end = 5\n",
    "ts = np.linspace(0, t_end, 100)\n",
    "\n",
    "x = np.random.rand(2)\n",
    "\n",
    "ts = np.linspace(0, t_end, 100)\n",
    "sol = solve_ivp(neurons.dh_dt, [0, t_end], neurons.activations, args=(x, neurons.weights), t_eval = ts)  \n",
    "\n",
    "w_inh = neurons.w_inh\n",
    "\n",
    "neurons.w_inh = 0\n",
    "sol2 = solve_ivp(neurons.dh_dt, [0, t_end], neurons.activations, args=(x, neurons.weights), t_eval = ts)  \n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=latex_figsize(wf=1.2, hf=.5, columnwidth=318.67))\n",
    "    fig.patch.set_facecolor('none')\n",
    "    \n",
    "    currents = neurons.currents(x, weights=neurons.weights, lebesgue_p=neurons.lebesgue_p)\n",
    "    sort_currents = np.argsort(currents)\n",
    "    stat_activations = neurons.stationary_activations(x, neurons.weights)[sort_currents]\n",
    "\n",
    "    colors = plt.cm.plasma((stat_activations - np.min(stat_activations)) / (np.max(stat_activations) - np.min(stat_activations)))\n",
    "\n",
    "    for i, solution in enumerate(sol.y[sort_currents]):\n",
    "        ax1.plot(ts, solution, color=colors[i], lw=1)\n",
    "    \n",
    "    for i, solution in enumerate(sol2.y[sort_currents]):\n",
    "        ax2.plot(ts, solution, color=colors[i], lw=1)\n",
    "\n",
    "    ax1.set_xlabel('Time $t$')\n",
    "    ax2.set_xlabel('Time $t$')\n",
    "    ax1.set_ylabel(r'Activation $h_\\mu$')\n",
    "    ax1.set_title(f\"(w\\\\_inh={w_inh}, p={neurons.lebesgue_p})\", fontsize=8, y=.97)\n",
    "    ax2.set_title(f\"(w\\\\_inh=0, p={neurons.lebesgue_p})\", fontsize=8, y=.97)\n",
    "    plt.suptitle(\"Neuron Dynamics\", x=.51, y=.99)\n",
    "    \n",
    "    ax1.patch.set_facecolor('white')\n",
    "    ax2.patch.set_facecolor('white')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.savefig(\"Plots/neuron_dynamics.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sweet-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with MNIST dataset\n",
    "\n",
    "try: #Load MNIST dataset\n",
    "    X = np.load(\"MNIST_features.npy\", allow_pickle=True)\n",
    "    y = np.load(\"MNIST_labels.npy\", allow_pickle=True)\n",
    "except IOError: #If not present, download it from the net\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False) #Return tuple (X=features, y=labels) as numpy array\n",
    "    #(as_frame=False => do not use Pandas DataFrame)\n",
    "\n",
    "    np.save(\"MNIST_features.npy\", X)\n",
    "    np.save(\"MNIST_labels.npy\", y)\n",
    "\n",
    "X = X/255. #Apply normalization\n",
    "\n",
    "class RandomPicker():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __call__(self):\n",
    "        return random.choice(self.dataset)\n",
    "    \n",
    "mnist_sample = RandomPicker(X)\n",
    "\n",
    "def draw_mnist_sample(data):\n",
    "    \"\"\"\n",
    "    Draw a sample from MNIST\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(data.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convinced-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeurons(input_dim=784, output_dim=15, w_inh=15, delta=0.4, lebesgue_p=2.0, h_star=0.8, tau=1, tau_L=100)\n"
     ]
    }
   ],
   "source": [
    "#Weights evolution\n",
    "neurons = BioNeurons(output_dim=15, input_dim=28**2, lebesgue_p=2., delta=0.4, h_star=.8, w_inh=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "global-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentange of non-zero deltas: 6.67%\n"
     ]
    }
   ],
   "source": [
    "eval_time = 5\n",
    "x = mnist_sample()\n",
    "\n",
    "#In general, not all weights are updated at every iteration\n",
    "#For the given hyperparameters (e.g. delta, w_inh, lebesgue_p...) the percentage of weights that are changed\n",
    "#in the first iteration is:\n",
    "non_zero_deltas_percentage = len(np.flatnonzero(neurons.dW_dt(0., neurons.weights, x, flatten=True))) / len(neurons.weights.flatten()) * 100 \n",
    "#Given the formula of dW_dt, this is the same percentage of non-zero post-activations\n",
    "print(\"Percentange of non-zero deltas: {:.2f}%\".format(non_zero_deltas_percentage))\n",
    "\n",
    "#Empirically, the model is most likely to converge (on the MNIST dataset) if this number is around 5-15%. \n",
    "#A higher number means that there are \"too many neurons active at once\", and w_inh should be set higher to have a higher lateral inhibition\n",
    "#Otherwise, neurons \"won't diversify much\", and are likely to converge to very similar patterns.\n",
    "#A lower number means that inhibition is too high, and so the model will take long to converge, or not converge at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "general-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "%time sol = solve_ivp(neurons.dW_dt, [0, eval_time], neurons.weights.flatten(), args=(x, True), t_eval = [eval_time]) \n",
    "#Measure time of an iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extreme-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(weights : np.ndarray,\n",
    "                 reshape_dim : tuple[int, int] = (28, 28),\n",
    "                 max_per_row : int = 5,\n",
    "                 max_rows : int = 5,\n",
    "                 fig = None): \n",
    "    \"\"\"\n",
    "    Plot the first few weights as matrices. `weights` should be an array of shape (output_dim, input_dim), i.e.\n",
    "    `weights[i,j]` is the weight connecting the $j$-th neuron of a layer $n$ to the $i$-th neuron of the $n+1$ layer.\n",
    "    Namely, all the weights connected to the $i$-th output neuron are the ones in the $i$-th row of `weights`.\n",
    "    These weights are reshaped according to `reshape_dim` to construct a matrix. The weight matrices of the first neurons\n",
    "    are then plotted in a grid of up to `max_rows` rows and `max_per_row` columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    #Shape of weights is (output_dim, input_dim)\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.figure()\n",
    "    else:\n",
    "        plt.clf()\n",
    "    \n",
    "    nc = np.max(np.abs(weights)) #(Absolute) range of weights\n",
    "    \n",
    "    n_neurons = weights.shape[0] \n",
    "    \n",
    "    #---Infer number of rows/columns---#\n",
    "    n_columns = max_per_row\n",
    "    n_rows = n_neurons // max_per_row\n",
    "    \n",
    "    if n_rows > max_rows:\n",
    "        n_rows = max_rows\n",
    "    if n_rows == 1:\n",
    "        n_columns = n_neurons\n",
    "    if n_neurons > max_rows * max_per_row:\n",
    "        n_neurons = max_rows * max_per_row\n",
    "    \n",
    "    #---Generate grid---#\n",
    "    whole_image = np.zeros(reshape_dim * np.array([n_rows, n_columns]))\n",
    "    \n",
    "    i_row = 0\n",
    "    i_col = 0\n",
    "    size_x, size_y = reshape_dim\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for index_neuron in range(n_neurons):\n",
    "        img = weights[index_neuron,...].reshape(reshape_dim)\n",
    "        whole_image[i_row * size_x:(i_row+1) * size_x,i_col * size_y:(i_col+1) * size_y] = img\n",
    "        i_col += 1\n",
    "        \n",
    "        if (i_col >= n_columns):\n",
    "            i_col = 0\n",
    "            i_row += 1\n",
    "    \n",
    "    #---Plot---#\n",
    "    img_plotted = plt.imshow(whole_image, cmap='bwr', vmin=-nc, vmax=nc, interpolation=None)\n",
    "    fig.colorbar(img_plotted,ticks=[np.amin(whole_image), 0, np.amax(whole_image)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop. ATTENTION! This cell takes VERY LONG to execute (30 min on my PC). \n",
    "#To just see the results, skip this cell and load the saved weights.\n",
    "\n",
    "num_trials = 300\n",
    "\n",
    "#Each sample is \"shown\" to the model for a time `eval_time`, which starts at `eval_time_max` and decreases\n",
    "#linearly to `eval_time_min`. The intuition is that, at first, the model needs to \"learn from scratch\", and\n",
    "#so we go \"more slowly\". Recall that the timescale of plasticity is, by default, 100. Intuitively, the evolution of each weight\n",
    "#takes into consideration mostly the samples falling inside this timescale. At the start, we want these to be few, \n",
    "#so that the neurons \"don't get too much confused\" and can stick to some specific sample.\n",
    "#Then, when weights are mostly fixed, we can \"go faster\" and have a lower `eval_time`. \n",
    "\n",
    "#This procedure is motivated by numerical experiments, and is roughly analogous to \"lowering the learning rate\"\n",
    "#during training of a supervised learning.\n",
    "\n",
    "eval_time_max = 25. \n",
    "eval_time_min = 5\n",
    "\n",
    "all_norms = []\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    x = mnist_sample()\n",
    "    \n",
    "    m = (eval_time_max - eval_time_min) / num_trials\n",
    "    eval_time = eval_time_max - m * i #Time of presentation of a single sample\n",
    "    \n",
    "    sol = solve_ivp(neurons.dW_dt, [0, eval_time], neurons.weights.flatten(), args=(x, True), t_eval = [eval_time])\n",
    "    \n",
    "    neurons.weights = sol.y.reshape(*neurons.weights.shape)\n",
    "    norms = np.sum(np.abs(neurons.weights) ** neurons.lebesgue_p, axis=1)\n",
    "    \n",
    "    all_norms.append(norms)\n",
    "    \n",
    "    draw_weights(neurons.weights, fig=fig)\n",
    "    \n",
    "    plt.savefig(f\"figs/weights{i}.png\", transparent=True, bbox_inches='tight') #Save images for animation\n",
    "\n",
    "#Save weights\n",
    "with open('bio_diffeq_converging', 'wb') as file:\n",
    "    pickle.dump(neurons, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cloudy-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc52c4ab3e20473d93895dd76d709b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load saved weights and show them\n",
    "with open('bio_diffeq_converging', 'rb') as file:\n",
    "    neurons = pickle.load(file)\n",
    "    \n",
    "draw_weights(neurons.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
